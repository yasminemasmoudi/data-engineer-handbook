# Product Experimentation on LeetCode

## User Journey (LeetCode)

### 1. Starting Point
When I first started using LeetCode, I was looking for a platform to practice coding problems and prepare for technical interviews. I was drawn to LeetCode because of its extensive library of problems categorized by difficulty and topic. The platform's reputation among job seekers and coding enthusiasts made it a natural choice for improving my algorithmic and problem-solving skills.

### 2. First Impressions
Initially, I found the interface intuitive and appreciated the ability to filter problems by tags and companies. The platform's clear structure allowed me to identify my weak areas and focus on targeted practice. The online code editor with multi-language support made it convenient to experiment with solutions in different programming languages. 

I also valued the instant feedback mechanism, which provided runtime errors, compilation results, and test case outcomes. The discussion forums further enhanced my experience by offering alternate solutions and explanations, making it easier to learn from others' perspectives.

### 3. Current Use
Now, I use LeetCode primarily to prepare for coding interviews and improve problem-solving and SQL skills. I actively participate in weekly contests to simulate real-time coding pressure, which helps build confidence. 

Additionally, I use the progress tracking features to monitor my performance and identify areas where I need improvement. The problem difficulty progression (Easy, Medium, Hard) helps me gradually increase my skill level without feeling overwhelmed.

## Experiments

### Experiment 1: Dynamic Problem Recommendations

#### Hypothesis
Providing dynamic recommendations based on recent performance and problem-solving patterns will help users stay engaged and improve their skills faster.

#### Test Design
- **Control Group:** Users continue using the current recommendation system based on manually selected tags and preferences.
- **Test Group A:** Users receive dynamic recommendations based on recent performance trends (e.g., prioritizing problems related to topics where the user struggles).
- **Test Group B:** Users receive personalized challenges updated daily, factoring in previous attempts, completion times, and skipped problems.

#### Test Cell Allocation
- **Control Group:** 50% of users randomly selected.
- **Test Group A:** 25% of users.
- **Test Group B:** 25% of users.

#### Metrics:
- **Leading Metric:** Increase in the number of problems attempted and solved per week.
- **Lagging Metric:** Improvement in problem-solving streaks, user retention rate, and time spent solving problems.


### Experiment 2: Hints and Partial Solutions Feature

#### Hypothesis
Adding hints and partial solutions for premium users will reduce frustration and improve problem completion rates, encouraging users to upgrade to premium memberships.

#### Test Design
- **Control Group:** Users do not have access to hints or partial solutions.
- **Test Group A:** Users receive contextual hints after the first failed attempt, such as explanations of the algorithm type or example patterns.
- **Test Group B:** Users are provided with partial code templates for complex problems (e.g., function signatures, key logic stubs) after the first failed attempt.

#### Test Cell Allocation
- **Control Group:** 40% of users.
- **Test Group A:** 30% of users.
- **Test Group B:** 30% of users.

#### Metrics:
- **Leading Metric:** Problem completion rate, bounce rate reduction, and time spent per problem.
- **Lagging Metric:** Growth in premium subscriptions and increased user satisfaction ratings from surveys.


### Experiment 3: Interactive Mock Interviews

#### Hypothesis
Introducing AI-powered interactive mock interviews will better prepare users for real technical interviews, increase platform usage, and attract new users.

#### Test Design
- **Control Group:** Users use the existing mock interview feature with static questions.
- **Test Group A:** Users experience AI-powered interviews that adapt dynamically based on real-time performance and errors.
- **Test Group B:** Users can schedule peer-to-peer mock interviews with built-in feedback forms for collaborative preparation.

#### Test Cell Allocation
- **Control Group:** 33% of users.
- **Test Group A:** 33% of users.
- **Test Group B:** 33% of users.

#### Metrics:
- **Leading Metric:** Increase in mock interview completions and feedback submissions.
- **Lagging Metric:** Growth in premium memberships, higher retention rates, and user-reported interview success statistics.


